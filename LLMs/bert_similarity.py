# -*- coding: utf-8 -*-
"""Bert-similarity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gpedknRQRT9cXlADDbEr-IWG07TAbWpc
"""

from transformers import BertTokenizer, BertModel
import torch

# Load pre-trained BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Set the model to evaluation mode (since we are not training it)
model.eval()

def get_bert_embedding(text, tokenizer, model):
    # Tokenize the input text
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)

    # Get the outputs from the model
    with torch.no_grad():
        outputs = model(**inputs)

    # The output embeddings are in the first element of the tuple, take the [CLS] token
    embeddings = outputs.last_hidden_state[:, 0, :]

    return embeddings.squeeze()  # Remove extra dimensions

from sklearn.metrics.pairwise import cosine_similarity

def compute_similarity(embedding1, embedding2):
    # Convert embeddings to 2D arrays if necessary
    embedding1 = embedding1.unsqueeze(0) if len(embedding1.shape) == 1 else embedding1
    embedding2 = embedding2.unsqueeze(0) if len(embedding2.shape) == 1 else embedding2

    # Calculate cosine similarity
    return cosine_similarity(embedding1, embedding2)[0][0]

def rank_entity_labels_by_relation(entity_labels, relation, tokenizer, model):
    # Get BERT embedding for the relation
    relation_embedding = get_bert_embedding(relation, tokenizer, model)

    # Compute similarities for each entity label
    label_similarities = []
    for label in entity_labels:
        label_embedding = get_bert_embedding(label, tokenizer, model)
        similarity = compute_similarity(relation_embedding, label_embedding)
        label_similarities.append((label, similarity))

    # Sort labels by similarity score in descending order
    ranked_labels = sorted(label_similarities, key=lambda x: x[1], reverse=True)

    return ranked_labels

import pandas as pd

def load_entity_labels(file_path):


    df = pd.read_csv(file_path, sep=',', header=0, names=['class', 'classLabel','count'])

    return df

def rank_entity_labels_by_relation(df, relation, tokenizer, model):


    if not isinstance(relation, str):
        raise ValueError("The relation must be a string.")

    # Get BERT embedding for the relation
    relation_embedding = get_bert_embedding(relation, tokenizer, model)

    # Compute similarities for each label
    label_similarities = []
    for _, row in df.iterrows():
        label = str(row['classLabel']) if pd.notnull(row['classLabel']) else ""
        label_embedding = get_bert_embedding(label, tokenizer, model)
        similarity = compute_similarity(relation_embedding, label_embedding)
        label_similarities.append((row['class'], label, similarity))

    # Sort labels by similarity score in asc order
    ranked_labels = sorted(label_similarities, key=lambda x: x[2])
    return ranked_labels

file_path = "/content/query-result.csv"
entity_labels_df = load_entity_labels(file_path)

# the relation to compare the labels with
relation = "occupation"

# Rank the labels by similarity to the relation
ranked_labels = rank_entity_labels_by_relation(entity_labels_df, relation, tokenizer, model)

# @title
# Display the ranked labels with similarity scores
for entity_id, label, similarity in ranked_labels:
    if similarity[0][1] < 0.75:
        print(f"ID: {entity_id}, Label: {label}")